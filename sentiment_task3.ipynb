{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Homework 4: Sentiment Analysis - Task 3\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Names\n",
    "----\n",
    "Names: Kaan Tural, Arinjay Singh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 3: Train a Logistic Regression Model (20 points)\n",
    "----\n",
    "\n",
    "Using `sklearn`'s implementation of `LogisticRegression`, conduct a similar analysis on the performance of a Logistic Regression classifier on the provided data set.\n",
    "\n",
    "Using the `time` module, you'll compare and contrast how long it takes your home-grown BoW vectorizing function vs. `sklearn`'s `CountVectorizer`.\n",
    "\n",
    "Logistic regression is used for binary classification, but can be extended for multi-class classification\n",
    "\n",
    "Read more about logistic regression here - https://www.analyticsvidhya.com/blog/2021/08/conceptual-understanding-of-logistic-regression-for-data-science-beginners/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from collections import Counter\n",
    "import time\n",
    "import sentiment_utils as sutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define constants for the files we are using\n",
    "TRAIN_FILE = \"movie_reviews_train.txt\"\n",
    "DEV_FILE = \"movie_reviews_dev.txt\"\n",
    "\n",
    "# load in your data and make sure you understand the format\n",
    "# Do not print out too much so as to impede readability of your notebook\n",
    "train_tups = sutils.generate_tuples_from_file(TRAIN_FILE)\n",
    "dev_tups = sutils.generate_tuples_from_file(DEV_FILE)\n",
    "\n",
    "trainX, trainY = train_tups\n",
    "devX, devY = dev_tups\n",
    "\n",
    "# some variables you may want to use\n",
    "# BINARIZED = True\n",
    "# USE_COUNT_VECTORIZER = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the functions needed (here or in sentiment_utils.py) to create vectorized BoW representations\n",
    "# of your data. We recommend starting with a multinomial BoW representation.\n",
    "# Each training example should be represented as a sparse vector.\n",
    "\n",
    "\n",
    "def create_vectors(train_data, dev_data, binarized = False, use_count_vectorizer = False):\n",
    "    if use_count_vectorizer:\n",
    "        vectorizer = CountVectorizer(binary=binarized)\n",
    "        vectorized_train_data = vectorizer.fit_transform([' '.join(x) for x in train_data])\n",
    "        vectorized_dev_data = vectorizer.transform([' '.join(x) for x in dev_data])\n",
    "    else:\n",
    "        vocab = list(set([token for sublist in trainX for token in sublist]))\n",
    "        vectorized_train_data = sutils.featurize(vocab=vocab, data_to_be_featurized_X=train_data, binary=binarized)\n",
    "        vectorized_dev_data = sutils.featurize(vocab=vocab, data_to_be_featurized_X=dev_data, binary=binarized)\n",
    "        \n",
    "    return vectorized_train_data, vectorized_dev_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 30705\n",
      "That took: 2.9245779514312744 seconds\n"
     ]
    }
   ],
   "source": [
    "# how much time does it take to featurize the all data with your implementation?\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# featurize the data\n",
    "featurized_train_data, featurized_dev_data = create_vectors(train_data=trainX, dev_data=devX, binarized=False, use_count_vectorizer=False)\n",
    "\n",
    "end = time.time()\n",
    "print(\"Vocab size:\", len(featurized_train_data[0]))\n",
    "print(\"That took:\", end - start, \"seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 22596\n",
      "That took: 0.16157889366149902 seconds\n"
     ]
    }
   ],
   "source": [
    "# how much time does it take to featurize the all data with sklearn's CountVectorizer?\n",
    "start = time.time()\n",
    "\n",
    "# featurize the data\n",
    "count_vectorizer_train_data, count_vectorizer_dev_data = create_vectors(train_data=trainX, dev_data=devX, binarized=False, use_count_vectorizer=True)\n",
    "\n",
    "\n",
    "end = time.time()\n",
    "print(\"Vocab size:\", count_vectorizer_train_data.shape[1])\n",
    "print(\"That took:\", end - start, \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. How big is your vocabulary using your vectorization function(s)? __The vocab size using the vectorization functions is 30705.__\n",
    "2. How big is your vocabulary using the `CountVectorizer`? __The vocab size using the CountVectorizer is 22596.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3508: SparseEfficiencyWarning: Comparing a sparse matrix with 0 using == is inefficient, try using != instead.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity of the vectors from featurize function: 99.51%\n",
      "Sparsity of the vectors from CountVectorizer: 99.39%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#  write any code you need analyze the relative sparsity of your vectorized representations of the data\n",
    "\n",
    "# featurize function\n",
    "data_array = np.array(featurized_train_data)\n",
    "sparsity = (data_array == 0).mean() * 100\n",
    "\n",
    "# CountVectorizer\n",
    "average_sparsity = (count_vectorizer_train_data == 0).mean(axis=1).mean() * 100\n",
    "\n",
    "# Print out the average % of entries that are zeros in each vector in the vectorized training data\n",
    "print(\"Sparsity of the vectors from featurize function: {:.2f}%\".format(sparsity))\n",
    "print(\"Sparsity of the vectors from CountVectorizer: {:.2f}%\".format(average_sparsity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the provided dev set, evaluate your model with precision, recall, and f1 score as well as accuracy\n",
    "# You may use nltk's implemented `precision`, `recall`, `f_measure`, and `accuracy` functions\n",
    "# (make sure to look at the documentation for these functions!)\n",
    "# you will be creating a similar graph for logistic regression and neural nets, so make sure\n",
    "# you use functions wisely so that you do not have excessive repeated code\n",
    "# write any helper functions you need in sentiment_utils.py (functions that you'll use in your other notebooks as well)\n",
    "\n",
    "\n",
    "# create a graph of your classifier's performance on the dev set as a function of the amount of training data\n",
    "# the x-axis should be the amount of training data (as a percentage of the total training data)\n",
    "# NOTE : make sure one of your experiments uses 10% of the data, you will need this to answer the first question in task 5\n",
    "# the y-axis should be the performance of the classifier on the dev set\n",
    "# the graph should have 4 lines, one for each of precision, recall, f1, and accuracy\n",
    "# the graph should have a legend, title, and axis labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My Vectorized Features, Multinomial F1 score: 0.780952380952381\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "featurized_multi_model = LogisticRegression(max_iter=250)\n",
    "featurized_multi_model.fit(featurized_train_data, trainY)\n",
    "featurized_multi_preds = featurized_multi_model.predict(featurized_dev_data)\n",
    "\n",
    "f1 = f1_score(devY, featurized_multi_preds)\n",
    "print(\"My Vectorized Features, Multinomial F1 score:\", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer, Multinomial F1 score: 0.7962962962962963\n"
     ]
    }
   ],
   "source": [
    "count_vectorizer_multi_model = LogisticRegression(max_iter=250)\n",
    "count_vectorizer_multi_model.fit(count_vectorizer_train_data, trainY)\n",
    "count_vectorizer_multi_preds = count_vectorizer_multi_model.predict(count_vectorizer_dev_data)\n",
    "\n",
    "f1 = f1_score(devY, count_vectorizer_multi_preds)\n",
    "print(\"CountVectorizer, Multinomial F1 score:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_featurized_train_data, bi_featurized_dev_data = create_vectors(train_data=trainX, dev_data=devX, binarized=True, use_count_vectorizer=False)\n",
    "bi_count_vectorizer_train_data, bi_count_vectorizer_dev_data = create_vectors(train_data=trainX, dev_data=devX, binarized=True, use_count_vectorizer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My Vectorized Features, Binarized F1 score: 0.81651376146789\n"
     ]
    }
   ],
   "source": [
    "featurized_binary_model = LogisticRegression(max_iter=250)\n",
    "featurized_binary_model.fit(bi_featurized_train_data, trainY)\n",
    "featurized_binary_preds = featurized_binary_model.predict(bi_featurized_dev_data)\n",
    "\n",
    "f1 = f1_score(devY, featurized_binary_preds)\n",
    "print(\"My Vectorized Features, Binarized F1 score:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer, Binarized F1 score: 0.8202764976958524\n"
     ]
    }
   ],
   "source": [
    "count_vectorizer_binary_model = LogisticRegression(max_iter=250)\n",
    "count_vectorizer_binary_model.fit(bi_count_vectorizer_train_data, trainY)\n",
    "count_vectorizer_binary_preds = count_vectorizer_binary_model.predict(bi_count_vectorizer_dev_data)\n",
    "\n",
    "f1 = f1_score(devY, count_vectorizer_binary_preds)\n",
    "print(\"CountVectorizer, Binarized F1 score:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the following 4 combinations to determine which has the best final f1 score for your Logistic Regression model:\n",
    "- your vectorized features, multinomial: __0.780952380952381__\n",
    "- CountVectorizer features, multinomial: __0.7962962962962963__\n",
    "- your vectorized features, binarized: __0.81651376146789__\n",
    "- CountVectorizer features, binarized: __0.8202764976958524__\n",
    "\n",
    "Produce your graph(s) for the combination with the best final f1 score.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3508, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/ml/0qpf6m6x3kbc0_wvk2pcsr6c0000gn/T/ipykernel_12907/160536019.py\", line 15, in <module>\n",
      "    sutils.create_training_graph(train_sizes, metrics, \"Logisitc Regression: CounterVectorizer Binarized\", f\"LR_graph_{i}.png\")\n",
      "  File \"/Users/arinjay/Documents/GitHub/Sentiment_Analysis_HW4/sentiment_utils.py\", line 103, in create_training_graph\n",
      "  File \"/Users/arinjay/Documents/GitHub/Sentiment_Analysis_HW4/sentiment_utils.py\", line 103, in <listcomp>\n",
      "ValueError: not enough values to unpack (expected 2, got 1)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 2105, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1428, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1319, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1172, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1087, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 969, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 792, in lines\n",
      "    return self._sd.lines\n",
      "  File \"/Users/arinjay/Library/Python/3.10/lib/python/site-packages/stack_data/utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/Users/arinjay/Library/Python/3.10/lib/python/site-packages/stack_data/core.py\", line 734, in lines\n",
      "    pieces = self.included_pieces\n",
      "  File \"/Users/arinjay/Library/Python/3.10/lib/python/site-packages/stack_data/utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/Users/arinjay/Library/Python/3.10/lib/python/site-packages/stack_data/core.py\", line 681, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "  File \"/Users/arinjay/Library/Python/3.10/lib/python/site-packages/stack_data/utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/Users/arinjay/Library/Python/3.10/lib/python/site-packages/stack_data/core.py\", line 660, in executing_piece\n",
      "    return only(\n",
      "  File \"/Users/arinjay/Library/Python/3.10/lib/python/site-packages/executing/executing.py\", line 190, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_sizes = np.linspace(0.1, 0.9, 9)\n",
    "metrics = list()\n",
    "\n",
    "for train_size in train_sizes:  \n",
    "    X_train, _, y_train, _ = train_test_split(bi_count_vectorizer_train_data, trainY, train_size=train_size, random_state=42)\n",
    "    model = LogisticRegression(max_iter=250)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(bi_count_vectorizer_dev_data)\n",
    "    current_metrics = sutils.get_prfa(devY, preds)\n",
    "    metrics.append(current_metrics)\n",
    "    \n",
    "for i in range(1, 4):\n",
    "    sutils.create_training_graph(train_sizes, metrics, \"Logisitc Regression: CounterVectorizer Binarized\", f\"LR_graph_{i}.png\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
