{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Homework 4: Sentiment Analysis - Task 0, Task 1, Task 5 (all primarily written tasks)\n",
    "----\n",
    "\n",
    "The following instructions are only written in this notebook but apply to all notebooks and `.py` files you submit for this homework.\n",
    "\n",
    "Due date: February 28th, 2024\n",
    "\n",
    "Points: \n",
    "- Task 0: 5 points\n",
    "- Task 1: 10 points\n",
    "- Task 2: 30 points\n",
    "- Task 3: 20 points\n",
    "- Task 4: 20 points\n",
    "- Task 5: 15 points\n",
    "\n",
    "Goals:\n",
    "- understand the difficulties of counting and probablities in NLP applications\n",
    "- work with real world data to build a functioning language model\n",
    "- stress test your model (to some extent)\n",
    "\n",
    "Complete in groups of: __two (pairs)__. If you prefer to work on your own, you may, but be aware that this homework has been designed as a partner project.\n",
    "\n",
    "Allowed python modules:\n",
    "- `numpy`, `matplotlib`, `keras`, `pytorch`, `nltk`, `pandas`, `sci-kit learn` (`sklearn`), `seaborn`, and all built-in python libraries (e.g. `math` and `string`)\n",
    "- if you would like to use a library not on this list, post on piazza to request permission\n",
    "- all *necessary* imports have been included for you (all imports that we used in our solution)\n",
    "\n",
    "Instructions:\n",
    "- Complete outlined problems in this notebook. \n",
    "- When you have finished, __clear the kernel__ and __run__ your notebook \"fresh\" from top to bottom. Ensure that there are __no errors__. \n",
    "    - If a problem asks for you to write code that does result in an error (as in, the answer to the problem is an error), leave the code in your notebook but commented out so that running from top to bottom does not result in any errors.\n",
    "- Double check that you have completed Task 0.\n",
    "- Submit your work on Gradescope.\n",
    "- Double check that your submission on Gradescope looks like you believe it should __and__ that all partners are included (for partner work).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 0: Name, References, Reflection (5 points)\n",
    "---\n",
    "\n",
    "Names\n",
    "----\n",
    "Names: Kaan Tural, Arinjay Singh\n",
    "\n",
    "References\n",
    "---\n",
    "List the resources you consulted to complete this homework here. Write one sentence per resource about what it provided to you. If you consulted no references to complete your assignment, write a brief sentence stating that this is the case and why it was the case for you.\n",
    "\n",
    "- https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\n",
    "    - The training data and dev data were sub-sampled from this source.\n",
    "\n",
    "\n",
    "Reflection\n",
    "----\n",
    "Answer the following questions __after__ you complete this assignment (no more than 1 sentence per question required, this section is graded on completion):\n",
    "\n",
    "1. Does this work reflect your best effort?\n",
    "\n",
    "Yes, this work does reflect our best effort.\n",
    "\n",
    "2. What was/were the most challenging part(s) of the assignment?\n",
    "\n",
    "The most challenging part of the assignment was training the Naive Bayes Classifier.\n",
    "\n",
    "3. If you want feedback, what function(s) or problem(s) would you like feedback on and why?\n",
    "\n",
    "I would like feedback about our feedforward neural net design because it is still a relatively new concept to us.\n",
    "\n",
    "4. Briefly reflect on how your partnership functioned--who did which tasks, how was the workload on each of you individually as compared to the previous homeworks, etc.\n",
    "\n",
    "The partnership worked well, Kaan worked on Tasks 1 and 2 while Arinjay worked on Tasks 3 and 4. Task 5 was completed together. The workload was relatively manageable compared to the previous homeworks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 1: Provided Data Write-Up (10 points)\n",
    "---\n",
    "\n",
    "Every time you use a data set in an NLP application (or in any software application), you should be able to answer a set of questions about that data. Answer these now. Default to no more than 1 sentence per question needed. If more explanation is necessary, do give it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is about the __provided__ movie review data set.\n",
    "\n",
    "1. Where did you get the data from? \n",
    "\n",
    "The provided dataset(s) were sub-sampled from https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews \n",
    "\n",
    "2. (1 pt) How was the data collected (where did the people acquiring the data get it from and how)?\n",
    "\n",
    "The data was collected from the Internet Movie Database (IMDB), however, it is not specified how the data was acquired. The discussions regarding the dataset in Kaggle seem to suggest that the data was retrieved using a webscraper.\n",
    "\n",
    "3. (2 pts) How large is the dataset (answer for both the train and the dev set, separately)? (# reviews, # tokens in both the train and dev sets)\n",
    "\n",
    "The dataset's train and test sets have 25,000 reviews each. The sub-sample used in this assignment has a train set of 1600 reviews (425421 tokens) and a dev set of 200 reviews (54603 tokens).\n",
    "\n",
    "4. (1 pt) What is your data? (i.e. newswire, tweets, books, blogs, etc)\n",
    "\n",
    "The data is made up of movie reviews that are considered highly polar.\n",
    "\n",
    "5. (1 pt) Who produced the data? (who were the authors of the text? Your answer might be a specific person or a particular group of people)\n",
    "\n",
    "The authors of the data are the reviewers that post their opinions about movies on IMDB.\n",
    "\n",
    "6. (2 pts) What is the distribution of labels in the data (answer for both the train and the dev set, separately)?\n",
    "\n",
    "The training data has 804 (50.25%) positive reviews labeled as 1 and 796 (40.75%) negative reviews labeled as 0. The dev set has 105 (52.5%) positive reviews labeled as 1 and 95 (47.5%) negative reviews labeled as 0.\n",
    "\n",
    "7. (2 pts) How large is the vocabulary (answer for both the train and the dev set, separately)?\n",
    "\n",
    "The vocabulary sizes of the train and the dev sets were 30705 and 8953 respectively.\n",
    "\n",
    "8. (1 pt) How big is the overlap between the vocabulary for the train and dev set?\n",
    "\n",
    "The vocabulary overlap between the train and dev set was 6574."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/arinjay/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# our utility functions\n",
    "# RESTART your jupyter notebook kernel if you make changes to this file\n",
    "import sentiment_utils as sutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary:\n",
      "\n",
      "Training Vocab Size: 30705\n",
      "Dev Vocab Size: 8953\n",
      "Overlap Vocab Size: 6574\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_reviews, train_labels = sutils.generate_tuples_from_file('movie_reviews_train.txt')\n",
    "dev_reviews, dev_labels = sutils.generate_tuples_from_file('movie_reviews_dev.txt')\n",
    "\n",
    "train_df = pd.DataFrame({'Review': train_reviews, 'Label': train_labels})\n",
    "dev_df = pd.DataFrame({'Review': dev_reviews, 'Label': dev_labels})\n",
    "\n",
    "# Vocabulary\n",
    "print(\"Vocabulary:\\n\")\n",
    "\n",
    "train_vocab = train_df['Review'].explode().unique()\n",
    "train_vocab_size = len(train_vocab)\n",
    "print(f\"Training Vocab Size: {train_vocab_size}\")\n",
    "\n",
    "dev_vocab = dev_df['Review'].explode().unique()\n",
    "dev_vocab_size = len(dev_vocab)\n",
    "print(f\"Dev Vocab Size: {dev_vocab_size}\")\n",
    "\n",
    "overlap_vocab = set(train_vocab) & set(dev_vocab)\n",
    "overlap_vocab_size = len(overlap_vocab)\n",
    "print(f\"Overlap Vocab Size: {overlap_vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Size:\n",
      "\n",
      "Training Size: 1600\n",
      "Dev Size: 200\n",
      "Training Tokens: 425421\n",
      "Dev Tokens: 54603\n"
     ]
    }
   ],
   "source": [
    "# Dataset Size\n",
    "print(\"Dataset Size:\\n\")\n",
    "\n",
    "train_size = len(train_df)\n",
    "print(f\"Training Size: {train_size}\")\n",
    "\n",
    "dev_size = len(dev_df)\n",
    "print(f\"Dev Size: {dev_size}\")\n",
    "\n",
    "train_tokens = train_df['Review'].apply(len).sum()\n",
    "print(f\"Training Tokens: {train_tokens}\")\n",
    "\n",
    "dev_tokens = dev_df['Review'].apply(len).sum()\n",
    "print(f\"Dev Tokens: {dev_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Distribution:\n",
      "\n",
      "Training Label Distribution:\n",
      "1    804\n",
      "0    796\n",
      "Name: Label, dtype: int64\n",
      "1    50.25\n",
      "0    49.75\n",
      "Name: Label, dtype: float64\n",
      "\n",
      "Dev Label Distribution:\n",
      "1    105\n",
      "0     95\n",
      "Name: Label, dtype: int64\n",
      "1    52.5\n",
      "0    47.5\n",
      "Name: Label, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Class Distribution\n",
    "print(\"Class Distribution:\\n\")\n",
    "\n",
    "label_distribution_train = train_df['Label'].value_counts()\n",
    "label_distribution_dev = dev_df['Label'].value_counts()\n",
    "\n",
    "total_train_samples = label_distribution_train.sum()\n",
    "total_dev_samples = label_distribution_dev.sum()\n",
    "\n",
    "label_distribution_train_percent = label_distribution_train / total_train_samples * 100\n",
    "label_distribution_dev_percent = label_distribution_dev / total_dev_samples * 100\n",
    "\n",
    "print(\"Training Label Distribution:\")\n",
    "print(label_distribution_train)\n",
    "print(label_distribution_train_percent)\n",
    "\n",
    "print(\"\\nDev Label Distribution:\")\n",
    "print(label_distribution_dev)\n",
    "print(label_distribution_dev_percent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 5: Model Evaluation (15 points)\n",
    "---\n",
    "Save your three graph files for the __best__ configurations that you found with your models using the `plt.savefig(filename)` command. The `bbox_inches` optional parameter will help you control how much whitespace outside of the graph is in your resulting image.\n",
    "\n",
    "__NOTE:__ Run each notebook containing a classifier 3 times, resulting in __NINE__ saved graphs (don't just overwrite your previous ones).\n",
    "\n",
    "You will turn in all of these files.\n",
    "\n",
    "10 points in this section are allocated for having all nine graphs legible, properly labeled, and present.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "1. (1 pt) When using __10%__ of your data, which model had the highest f1 score?\n",
    "\n",
    "The feedforward neural net had the highest f1 score when using 10% of the training data at 0.685.\n",
    "\n",
    "2. (1 pt) Which classifier had the most __consistent__ performance (that is, which classifier had the least variation across all three graphs you have for it -- no need to mathematically calculate this, you can just look at the graphs)?\n",
    "\n",
    "The Logistic Regression model had the most consistent performance as all three of its graphs were more or less identical.\n",
    "\n",
    "3. (1 pt) For each model, what percentage of training data resulted in the highest f1 score?\n",
    "    1. Naive Bayes:\n",
    "    2. Logistic Regression: 80%\n",
    "    3. Neural Net: 70%\n",
    "\n",
    "4. (2 pts) Which model, if any, appeared to overfit the training data the most? Why?\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
